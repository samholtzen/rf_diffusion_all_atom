{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e578ba7a-6cc5-4cb7-9bd8-d63dbaba38af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os, sys, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "from shutil import copy2\n",
    "import random\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "### Path to this cloned GitHub repo:\n",
    "SCRIPT_DIR = os.path.dirname('/home/sholtzen/heme_binder_diffusion/')  # edit this to the GitHub repo path. Throws an error by default.\n",
    "assert os.path.exists(SCRIPT_DIR)\n",
    "sys.path.append(f\"/home/sholtzen/heme_binder_diffusion/scripts/utils/\")\n",
    "import utils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25115bb",
   "metadata": {},
   "source": [
    "The pipeline consists of 7 steps:<br>\n",
    "\n",
    "    0) The protein backbones are generated with RFdiffusionAA\n",
    "    1) Sequence is designed with proteinMPNN (without the ligand)\n",
    "    2) Structures are predicted with AlphaFold2\n",
    "    3) Ligand binding site is designed with LigandMPNN/FastRelax, or Rosetta FastDesign\n",
    "    4) Sequences surrounding the ligand pocket are diversified with LigandMPNN\n",
    "    5) Final designed sequences are predicted with AlphaFold2\n",
    "    6) Alphafold2-predicted models are relaxed with the ligand and analyzed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b49d9e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_script = f\"{SCRIPT_DIR}/rf_diffusion_all_atom/run_inference.py\"  # edit this\n",
    "inpaint_script = f\"{SCRIPT_DIR}/RFDesign/inpainting/inpaint.py\"  # edit this if needed\n",
    "proteinMPNN_script = f\"{SCRIPT_DIR}/lib/LigandMPNN/run.py\"  # from submodule\n",
    "AF2_script = f\"{SCRIPT_DIR}/scripts/af2/af2.py\"  # from submodule\n",
    "\n",
    "### Python and/or Apptainer executables needed for running the jobs\n",
    "### Please provide paths to executables that are able to run the different tasks.\n",
    "### They can all be the same if you have an environment with all of the ncessary Python modules in one\n",
    "\n",
    "# If your added Apptainer does not execute scripts directly,\n",
    "# try adding 'apptainer run' or 'apptainer run --nv' (for GPU) in front of the command\n",
    "\n",
    "CONDAPATH = \"/home/sholtzen/miniforge3\"   # edit this depending on where your Conda environments live\n",
    "PYTHON = {\"diffusion\": f\"{CONDAPATH}/envs/diffusion/bin/python\",\n",
    "          \"af2\": f\"{CONDAPATH}/envs/mlfold/bin/python\",\n",
    "          \"proteinMPNN\": f\"{CONDAPATH}/envs/diffusion/bin/python\",\n",
    "          \"general\": f\"{CONDAPATH}/envs/diffusion/bin/python\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d4990c-bff7-44dd-b675-c0379d9a45af",
   "metadata": {},
   "source": [
    "## Project description and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59cfb5a8-5f64-49f4-a26f-0da5f09b4120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /home/sholtzen/heme_binder_diffusion/ACO_binders/\n"
     ]
    }
   ],
   "source": [
    "username='sholtzen'\n",
    "LIGAND = \"ACO\"\n",
    "\n",
    "### Path where the jobs will be run and outputs dumped\n",
    "WDIR = f\"{SCRIPT_DIR}/{LIGAND}_binders/\"\n",
    "\n",
    "if not os.path.exists(WDIR):\n",
    "    os.makedirs(WDIR, exist_ok=True)\n",
    "\n",
    "print(f\"Working directory: {WDIR}\")\n",
    "\n",
    "USE_GPU_for_AF2 = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fca3ef-ba68-4896-8a98-dcc881832174",
   "metadata": {},
   "source": [
    "## 0: Setting up diffusion run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b9faacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 PDB files\n"
     ]
    }
   ],
   "source": [
    "# Using example PDB file with ligand HBA and protein 7o2g backbone.\n",
    "## Note: the repository also contains additional HBA conformers with 7o2g and P450 motifs\n",
    "## in the same directory as a ZIP file.\n",
    "\n",
    "params = [f\"{SCRIPT_DIR}/input/params/{LIGAND}/{LIGAND}.params\"]  # Rosetta params file(s)\n",
    "\n",
    "diffusion_inputs = glob.glob(f\"{SCRIPT_DIR}/input/pdbs/{LIGAND}/*_{LIGAND}.pdb\")\n",
    "\n",
    "diffusion_idx = random.sample(range(len(diffusion_inputs)), 1)\n",
    "diffusion_inputs = [diffusion_inputs[i] for i in diffusion_idx]\n",
    "\n",
    "print(f\"Found {len(diffusion_inputs)} PDB files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "84ea7254-ff5b-4588-bed3-5b05a50a4fdb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sholtzen/heme_binder_diffusion/ACO_binders/0_diffusion\n",
      "Estimated time to produce 1 designs = 4 minutes\n",
      "Wrote config file to /home/sholtzen/heme_binder_diffusion/ACO_binders/0_diffusion/config.yaml\n"
     ]
    }
   ],
   "source": [
    "### Setting up general settings for diffusion\n",
    "DIFFUSION_DIR = f\"{WDIR}0_diffusion\"\n",
    "print(DIFFUSION_DIR)\n",
    "\n",
    "if not os.path.exists(DIFFUSION_DIR):\n",
    "    os.makedirs(DIFFUSION_DIR, exist_ok=False)\n",
    "\n",
    "os.chdir(DIFFUSION_DIR)\n",
    "\n",
    "N_designs = 1\n",
    "T_steps = 200\n",
    "\n",
    "## Edit this config based on motif residues, etc...\n",
    "config = f\"\"\"\n",
    "defaults:\n",
    "  - aa\n",
    "  - _self_\n",
    "\n",
    "diffuser:\n",
    "  T: {T_steps}\n",
    "\n",
    "inference:\n",
    "  num_designs: {N_designs}\n",
    "  model_runner: NRBStyleSelfCond\n",
    "  ligand: '{LIGAND}'\n",
    "  cuda_core: 0\n",
    "\n",
    "model:\n",
    "  freeze_track_motif: True\n",
    "\n",
    "contigmap:\n",
    "  contigs: [\"120-170\"]\n",
    "  inpaint_str: null\n",
    "\n",
    "potentials:\n",
    "  guiding_potentials: [\"type:ligand_ncontacts,weight:2\"] \n",
    "  guide_scale: 2\n",
    "  guide_decay: quadratic\n",
    "\"\"\"\n",
    "\n",
    "# assuming 8 seconds per timestep on GTX 1080 and batch size of 3 across two GPUs\n",
    "estimated_time = 8 * T_steps * N_designs * len(diffusion_inputs) / 3 / 2\n",
    "\n",
    "print(f\"Estimated time to produce {N_designs * len(diffusion_inputs)} designs = {estimated_time/60:.0f} minutes\")\n",
    "with open(\"config.yaml\", \"w\") as file:\n",
    "    file.write(config)\n",
    "print(f\"Wrote config file to {os.path.realpath('config.yaml')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8a89617-a8a7-45d5-abd5-dc85fa32bc17",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 processes on core 0.\n",
      "0 processes on core 1.\n",
      "Diffusing backbone based on PDB 6TDG, running on CUDA core 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sholtzen/heme_binder_diffusion/rf_diffusion_all_atom/util.py:216: UserWarning: Using torch.cross without specifying the dim arg is deprecated.\n",
      "Please either pass the dim explicitly or simply use torch.linalg.cross.\n",
      "The default value of dim will change to agree with that of linalg.cross in a future release. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025831482/work/aten/src/ATen/native/Cross.cpp:63.)\n",
      "  Z = torch.cross(Xn,Yn)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sholtzen/heme_binder_diffusion/rf_diffusion_all_atom/run_inference.py\", line 237, in <module>\n",
      "    main()\n",
      "  File \"/home/sholtzen/miniforge3/envs/diffusion/lib/python3.9/site-packages/hydra/main.py\", line 94, in decorated_main\n",
      "    _run_hydra(\n",
      "  File \"/home/sholtzen/miniforge3/envs/diffusion/lib/python3.9/site-packages/hydra/_internal/utils.py\", line 394, in _run_hydra\n",
      "    _run_app(\n",
      "  File \"/home/sholtzen/miniforge3/envs/diffusion/lib/python3.9/site-packages/hydra/_internal/utils.py\", line 457, in _run_app\n",
      "    run_and_report(\n",
      "  File \"/home/sholtzen/miniforge3/envs/diffusion/lib/python3.9/site-packages/hydra/_internal/utils.py\", line 220, in run_and_report\n",
      "    return func()\n",
      "  File \"/home/sholtzen/miniforge3/envs/diffusion/lib/python3.9/site-packages/hydra/_internal/utils.py\", line 458, in <lambda>\n",
      "    lambda: hydra.run(\n",
      "  File \"/home/sholtzen/miniforge3/envs/diffusion/lib/python3.9/site-packages/hydra/_internal/hydra.py\", line 119, in run\n",
      "    ret = run_job(\n",
      "  File \"/home/sholtzen/miniforge3/envs/diffusion/lib/python3.9/site-packages/hydra/core/utils.py\", line 186, in run_job\n",
      "    ret.return_value = task_function(task_cfg)\n",
      "  File \"/home/sholtzen/heme_binder_diffusion/rf_diffusion_all_atom/run_inference.py\", line 77, in main\n",
      "    sample(sampler)\n",
      "  File \"/home/sholtzen/heme_binder_diffusion/rf_diffusion_all_atom/run_inference.py\", line 119, in sample\n",
      "    sampler_out = sample_one(sampler)\n",
      "  File \"/home/sholtzen/heme_binder_diffusion/rf_diffusion_all_atom/run_inference.py\", line 140, in sample_one\n",
      "    px0, x_t, seq_t, tors_t, plddt, rfo = sampler.sample_step(\n",
      "  File \"/home/sholtzen/heme_binder_diffusion/rf_diffusion_all_atom/inference/model_runners.py\", line 308, in sample_step\n",
      "    rfo = self.model_adaptor.forward(\n",
      "  File \"/home/sholtzen/heme_binder_diffusion/rf_diffusion_all_atom/aa_model.py\", line 256, in forward\n",
      "    a = self.model(**{**rfi_dict, **kwargs})\n",
      "  File \"/home/sholtzen/miniforge3/envs/diffusion/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/sholtzen/miniforge3/envs/diffusion/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/sholtzen/heme_binder_diffusion/rf_diffusion_all_atom/rf2aa/model/RoseTTAFoldModel.py\", line 368, in forward\n",
      "    msa, pair, xyz, alpha_s, xyz_allatom, state, symmsub, quat = self.simulator(\n",
      "  File \"/home/sholtzen/miniforge3/envs/diffusion/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/sholtzen/miniforge3/envs/diffusion/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/sholtzen/heme_binder_diffusion/rf_diffusion_all_atom/rf2aa/model/Track_module.py\", line 1132, in forward\n",
      "    msa, pair, xyz, state, alpha, symmsub, quat = self.main_block[i_m](msa, pair,\n",
      "  File \"/home/sholtzen/miniforge3/envs/diffusion/lib/python3.9/site-packages/torch/nn/modules/modul"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 71\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Sleep while all processes are running\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mall\u001b[39m([process\u001b[38;5;241m.\u001b[39mpoll() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m process \u001b[38;5;129;01min\u001b[39;00m processes]):\n\u001b[0;32m---> 71\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Once any process is done, identify that process as the index of the process list (and by extension, the cuda core it's assigned to)\u001b[39;00m\n\u001b[1;32m     74\u001b[0m done_cuda \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i, x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(processes) \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mpoll() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/sholtzen/miniforge3/envs/diffusion/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/sholtzen/heme_binder_diffusion/rf_diffusion_all_atom/rf2aa/model/Track_module.py\", line 963, in forward\n",
      "    xyz, state, alpha, quat = self.str2str(\n",
      "  File \"/home/sholtzen/miniforge3/envs/diffusion/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/sholtzen/miniforge3/envs/diffusion/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/sholtzen/miniforge3/envs/diffusion/lib/python3.9/site-packages/torch/amp/autocast_mode.py\", line 16, in decorate_autocast\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/sholtzen/heme_binder_diffusion/rf_diffusion_all_atom/rf2aa/model/Track_module.py\", line 518, in forward\n",
      "    neighbor = get_seqsep_protein_sm(idx, bond_feats, dist_matrix, rotation_mask)\n",
      "  File \"/home/sholtzen/heme_binder_diffusion/rf_diffusion_all_atom/rf2aa/util_module.py\", line 130, in get_seqsep_protein_sm\n",
      "    res_dist, atom_dist = get_res_atom_dist(idx, bond_feats, dist_matrix, sm_mask)\n",
      "  File \"/home/sholtzen/heme_binder_diffusion/rf_diffusion_all_atom/rf2aa/util_module.py\", line 180, in get_res_atom_dist\n",
      "    i_s, j_s = torch.where(bond_feats==6)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "## Setting up diffusion commands based on the input PDB file(s)\n",
    "## Diffusion jobs are run in separate directories for each input PDB\n",
    "from threadpoolctl import threadpool_limits, threadpool_info\n",
    "\n",
    "import time\n",
    "\n",
    "commands_diffusion = []\n",
    "cmds_filename = \"commands_diffusion\"\n",
    "diffusion_rundirs = []\n",
    "\n",
    "\n",
    "with open(cmds_filename, \"w\") as file:\n",
    "    for p in diffusion_inputs:\n",
    "        pdbname = os.path.basename(p).replace(\".pdb\", \"\")\n",
    "        os.makedirs(pdbname, exist_ok=True)\n",
    "        cmd = f\"cd {pdbname} ; {PYTHON['diffusion']} {diffusion_script} --config-dir={DIFFUSION_DIR} \"\\\n",
    "              f\"--config-name=config.yaml inference.input_pdb={p} \"\\\n",
    "              f\"inference.output_prefix='./out/{pdbname}_dif' \"\n",
    "        commands_diffusion.append(cmd)\n",
    "        diffusion_rundirs.append(pdbname)\n",
    "        file.write(cmd)\n",
    "\n",
    "        \n",
    "\n",
    "# Rudimentary CUDA scheduler...\n",
    "\n",
    "# Instantiate cuda and process lists\n",
    "\n",
    "batch_size = 10\n",
    "num_cuda_cores = [5,6]\n",
    "\n",
    "open_cuda = []\n",
    "taken_cuda = []\n",
    "processes = []\n",
    "\n",
    "# Create open cuda list with all avaialble cuda cores\n",
    "for cuda in num_cuda_cores:\n",
    "    for batch in range(batch_size):\n",
    "        open_cuda.append(cuda)  \n",
    "\n",
    "        \n",
    "# Iterate through `commands_diffusion` variable until there are no commands left\n",
    "while commands_diffusion:\n",
    "\n",
    "    # Take a census of the open CUDA to see that they're not at batch size\n",
    "    census_cuda = [taken_cuda.count(0), taken_cuda.count(1)]\n",
    "    for i, ele in enumerate(census_cuda):\n",
    "        print(f'{ele} processes on core {i}.') \n",
    "    cuda_maxed = all(ele==batch_size for ele in census_cuda)\n",
    "\n",
    "    # iterate through the open cuda cores for as long as there are open cores and they're not maxed\n",
    "    while open_cuda and not cuda_maxed and commands_diffusion:\n",
    "\n",
    "        # Pluck command and cuda core pair\n",
    "        command = commands_diffusion.pop(0)\n",
    "        cuda_core = open_cuda.pop(0)\n",
    "\n",
    "        # Assign that cuda core to the list of taken cores\n",
    "        taken_cuda.append(cuda_core)        \n",
    "\n",
    "        # Write commands with that assigned core and create a subprocess on that core\n",
    "        command += f\"inference.cuda_core={cuda_core} \"\n",
    "        command += \"> output.log ; cd ..\\n\"\n",
    "        with threadpool_limits(limits=1):\n",
    "            processes.append(subprocess.Popen(command, shell=True))\n",
    "\n",
    "        print(f'Diffusing backbone based on PDB {command[3:7]}, running on CUDA core {cuda_core}...')\n",
    "\n",
    "    # Sleep while all processes are running\n",
    "    while all([process.poll() is None for process in processes]):\n",
    "        time.sleep(1)\n",
    "\n",
    "    # Once any process is done, identify that process as the index of the process list (and by extension, the cuda core it's assigned to)\n",
    "    done_cuda = [i for i, x in enumerate(processes) if x.poll() is not None]\n",
    "\n",
    "    print(f'Process {done_cuda[0]} is done, switching...')\n",
    "\n",
    "    # Get out the index of that process's cuda core in the `taken_cuda` variable\n",
    "    index_cuda = done_cuda.pop(0)\n",
    "    processes.pop(index_cuda)\n",
    "\n",
    "    # Which cuda core are we talking about?\n",
    "    move_cuda = taken_cuda[index_cuda]\n",
    "\n",
    "    # Append `move_cuda` to `open_cuda` and remove that core from `taken_cuda`\n",
    "    open_cuda.append(move_cuda)\n",
    "    taken_cuda.pop(index_cuda)\n",
    "\n",
    "print('All done!')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ed7eb2-c045-49a0-9264-292a6ef4dc27",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## If you're done with diffusion and happy with the outputs then mark it as done\n",
    "DIFFUSION_DIR = f\"{WDIR}/0_diffusion\"\n",
    "os.chdir(DIFFUSION_DIR)\n",
    "\n",
    "if not os.path.exists(DIFFUSION_DIR+\"/.done\"):\n",
    "    with open(f\"{DIFFUSION_DIR}/.done\", \"w\") as file:\n",
    "        file.write(f\"Run user: {username}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a474a099-60e1-493d-85f2-12b8808a0b11",
   "metadata": {},
   "source": [
    "### Analyzing diffusion outputs\n",
    "The purpose of this step is to identify diffused backbones that meet certain quality criteria. These scaffolds should be relatively globular (measured by radius of gyration (rog), and longest helix). They should not have clashes between the ligand and the backbone, the ligand should not be too exposed (measured by relative SASA). The termini should not be too close to the ligand (term_mindist), and the backbone should not be too loopy. In the example below we are also looking for backbones that leave some part of the ligand more exposed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c85384-baa4-41c2-bc21-42587839a44d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Analyzing diffusion outputs for clashes, ligand burial and scaffold quality\n",
    "## If it's running too slowly consider increasing --nproc\n",
    "analysis_script = f\"{SCRIPT_DIR}/scripts/diffusion_analysis/process_diffusion_outputs.py\"\n",
    "\n",
    "diffusion_outputs = []\n",
    "for d in diffusion_rundirs:\n",
    "    diffusion_outputs += glob.glob(f\"{d}/out/*.pdb\")\n",
    "    \n",
    "# By default I don't use the --analyze flag. As a result the backbones are filtered as the script runs.\n",
    "# You can set --analyze to True to calculate all scores for all backbones.\n",
    "# This will slow the analysis down, but you can then filter the backbones separately afterwards.\n",
    "dif_analysis_cmd_dict = {\"--pdb\": \" \".join(diffusion_outputs),\n",
    "                        \"--params\": \" \".join(params),\n",
    "                        \"--term_limit\": \"10.0\",\n",
    "                        \"--SASA_limit\": \"0.4\",  # Highest allowed relative SASA of ligand\n",
    "                        \"--loop_limit\": \"0.4\",  # Fraction of backbone that can be loopy\n",
    "                        \"--rethread\": True,\n",
    "                        \"--longest_helix\": \"30\",\n",
    "                        \"--rog\": \"30.0\",\n",
    "                        \"--fix\": True,\n",
    "                        \"--partial\": None,\n",
    "                        \"--outdir\": None,\n",
    "                        \"--traj\": \"5/30\",  # Also random 5 models are taken from the last 30 steps of the diffusion trajectory\n",
    "                        \"--analyze\": False,\n",
    "                        \"--nproc\": \"1\"}\n",
    "\n",
    "analysis_command = f\"{PYTHON['general']} {analysis_script}\"\n",
    "\n",
    "for k, val in dif_analysis_cmd_dict.items():\n",
    "    if val is not None:\n",
    "        if isinstance(val, list):\n",
    "            analysis_command += f\" {k}\"\n",
    "            analysis_command += \" \" + \" \".join(val)\n",
    "        elif isinstance(val, bool):\n",
    "            if val == True:\n",
    "                analysis_command += f\" {k}\"\n",
    "        else:\n",
    "            analysis_command += f\" {k} {val}\"\n",
    "        print(k, val)\n",
    "\n",
    "p = subprocess.Popen(analysis_command, shell=True)\n",
    "(output, err) = p.communicate()\n",
    "\n",
    "diffused_backbones_good = glob.glob(f\"{DIFFUSION_DIR}/filtered_structures/*.pdb\")\n",
    "\n",
    "dif_analysis_df = pd.read_csv(f\"{DIFFUSION_DIR}/diffusion_analysis.sc\", header=0, sep=r\"\\s+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10b4314",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualizing the distributions of diffusion analysis metrics\n",
    "## Plotting design scores\n",
    "plt.figure(figsize=(12, 12))\n",
    "for i,k in enumerate(dif_analysis_df.keys()):\n",
    "    if k in [\"description\"]:\n",
    "        continue\n",
    "    plt.subplot(4, 3, i+1)\n",
    "    plt.hist(dif_analysis_df[k])\n",
    "    plt.title(k)\n",
    "    plt.xlabel(k)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6136b4",
   "metadata": {},
   "source": [
    "It is highly advised that you manually inspect the filtered diffusion outputs before continuing with the pipeline.\n",
    "While the filters attempt to pick out the most offending designs then nothing beats your own intuition and judgement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9561c720",
   "metadata": {},
   "source": [
    "If you would like to perform RFjoint Inpainting on the diffusion outputs, please go to the [inpainting section](#inpainting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7182f5c5-ab5c-4f98-b007-3f72da606147",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93a2f55",
   "metadata": {},
   "source": [
    "## 1: Running ProteinMPNN on diffused backbones\n",
    "\n",
    "We are first trying to just design a sequence on the backbone, without considering the ligand.\n",
    "The goal is to first find backbones that fold well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7235734c-e319-48cd-80e2-f6c5e0cacfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffused_backbones_good = glob.glob(f\"{DIFFUSION_DIR}/filtered_structures/*.pdb\")\n",
    "assert len(diffused_backbones_good) > 0, \"No good backbones found!\"\n",
    "\n",
    "os.chdir(WDIR)\n",
    "\n",
    "MPNN_DIR = f\"{WDIR}/1_proteinmpnn\"\n",
    "os.makedirs(MPNN_DIR, exist_ok=True)\n",
    "os.chdir(MPNN_DIR)\n",
    "\n",
    "### Parsing diffusion output TRB files to extract fixed motif residues\n",
    "## These residues will not be redesigned with proteinMPNN\n",
    "mask_json_cmd = f\"{PYTHON['general']} {SCRIPT_DIR}/scripts/design/make_maskdict_from_trb.py --out masked_pos.jsonl --trb\"\n",
    "for d in diffused_backbones_good:\n",
    "    mask_json_cmd += \" \" + d.replace(\".pdb\", \".trb\")\n",
    "p = subprocess.Popen(mask_json_cmd, shell=True)\n",
    "(output, err) = p.communicate()\n",
    "\n",
    "assert os.path.exists(\"masked_pos.jsonl\"), \"Failed to create masked positions JSONL file\"\n",
    "\n",
    "\n",
    "### Setting up proteinMPNN run commands\n",
    "## We're doing design with 3 temperatures, and 5 sequences each.\n",
    "## This usually gives decent success with designable backbones.\n",
    "## For more complicated cases consider doing >100 sequences.\n",
    "\n",
    "MPNN_temperatures = [0.1, 0.2, 0.3]\n",
    "MPNN_outputs_per_temperature = 5\n",
    "MPNN_omit_AAs = \"M\"\n",
    "\n",
    "commands_mpnn = []\n",
    "cmds_filename_mpnn = \"commands_mpnn\"\n",
    "with open(cmds_filename_mpnn, \"w\") as file:\n",
    "    for T in MPNN_temperatures:\n",
    "        for f in diffused_backbones_good:\n",
    "            commands_mpnn.append(f\"{PYTHON['proteinMPNN']} {proteinMPNN_script} \"\n",
    "                                 f\"--model_type protein_mpnn --ligand_mpnn_use_atom_context 0 \"\n",
    "                                 \"--fixed_residues_multi masked_pos.jsonl --out_folder ./ \"\n",
    "                                 f\"--number_of_batches {MPNN_outputs_per_temperature} --temperature {T} \"\n",
    "                                 f\"--omit_AA {MPNN_omit_AAs} --pdb_path {f} \"\n",
    "                                 f\"--checkpoint_protein_mpnn {SCRIPT_DIR}/lib/LigandMPNN/model_params/proteinmpnn_v_48_020.pt\\n\")\n",
    "            file.write(commands_mpnn[-1])\n",
    "\n",
    "print(\"Example MPNN command:\")\n",
    "print(commands_mpnn[-1])\n",
    "\n",
    "### Running proteinMPNN with Slurm.\n",
    "### Grouping jobs with 10 commands per one array job.\n",
    "with open(cmds_filename_mpnn, 'r') as file:\n",
    "    for line in file:\n",
    "        p = subprocess.Popen(line, shell=True)\n",
    "        (output, err) = p.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f793273",
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you're done with diffusion and happy with the outputs then mark it as done\n",
    "MPNN_DIR = f\"{WDIR}/1_proteinmpnn\"\n",
    "os.chdir(MPNN_DIR)\n",
    "\n",
    "if not os.path.exists(MPNN_DIR+\"/.done\"):\n",
    "    with open(f\"{MPNN_DIR}/.done\", \"w\") as file:\n",
    "        file.write(f\"Run user: {username}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97333709",
   "metadata": {},
   "source": [
    "## 2: Running AlphaFold2\n",
    "Performing AF2 single sequence predictions.<br>\n",
    "By default only using AF2 model 4 with 3 recycles. For more complicated folds 10+ recycles might be necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065029b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(WDIR)\n",
    "\n",
    "AF2_DIR = f\"{WDIR}/2_af2\"\n",
    "os.makedirs(AF2_DIR, exist_ok=True)\n",
    "os.chdir(AF2_DIR)\n",
    "\n",
    "### First collecting MPNN outputs and creating FASTA files for AF2 input\n",
    "mpnn_fasta = utils.parse_fasta_files(glob.glob(f\"{MPNN_DIR}/seqs/*.fa\"))\n",
    "mpnn_fasta = {k: seq.strip() for k, seq in mpnn_fasta.items() if \"model_path\" not in k}  # excluding the diffused poly-A sequence\n",
    "# Giving sequences unique names based on input PDB name, temperature, and sequence identifier\n",
    "mpnn_fasta = {k.split(\",\")[0]+\"_\"+k.split(\",\")[2].replace(\" T=\", \"T\")+\"_0_\"+k.split(\",\")[1].replace(\" id=\", \"\"): seq for k, seq in mpnn_fasta.items()}\n",
    "\n",
    "print(f\"A total on {len(mpnn_fasta)} sequences will be predicted.\")\n",
    "\n",
    "## Splitting the MPNN sequences based on length\n",
    "## and grouping them in smaller batches for each AF2 job\n",
    "## Use group size of >40 when running on GPU. Also depends on how many sequences and resources you have.\n",
    "\n",
    "with open('all_seq.fasta', 'w') as file:\n",
    "    for k in mpnn_fasta:\n",
    "        file.write(f\"{k}\\n{mpnn_fasta[k]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf52a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting up AlphaFold2 run\n",
    "\n",
    "AF2_recycles = 3\n",
    "AF2_models = \"4\"  # add other models to this string if needed, i.e. \"3 4 5\"\n",
    "\n",
    "commands_af2 = []\n",
    "cmds_filename_af2 = \"commands_af2\"\n",
    "with open(cmds_filename_af2, \"w\") as file:\n",
    "    for ff in glob.glob(\"*.fasta\"):\n",
    "        commands_af2.append(f\"{PYTHON['af2']} {AF2_script} \"\n",
    "                             f\"--af-nrecycles {AF2_recycles} --af-models {AF2_models} \"\n",
    "                             f\"--fasta {ff} --scorefile {ff.replace('.fasta', '.csv')}\\n\")\n",
    "        file.write(commands_af2[-1])\n",
    "\n",
    "print(\"Example AF2 command:\")\n",
    "print(commands_af2[-1]) \n",
    "\n",
    "with open(cmds_filename_af2, 'r') as file:\n",
    "    for line in file:\n",
    "        p = subprocess.Popen(line, shell=True)\n",
    "        (output, err) = p.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a88b24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you're done with diffusion and happy with the outputs then mark it as done\n",
    "AF2_DIR = f\"{WDIR}/2_af2\"\n",
    "os.chdir(AF2_DIR)\n",
    "\n",
    "if not os.path.exists(AF2_DIR+\"/.done\"):\n",
    "    with open(f\"{AF2_DIR}/.done\", \"w\") as file:\n",
    "        file.write(f\"Run user: {username}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242e9def",
   "metadata": {},
   "source": [
    "### Analyzing AlphaFold2 predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f38048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining all CSV scorefiles into one\n",
    "AF2_DIR = f\"{WDIR}/2_af2\"\n",
    "DIFFUSION_DIR = f\"{WDIR}/0_diffusion\"\n",
    "\n",
    "os.system(\"head -n 1 $(ls all_seq.csv | shuf -n 1) > scores.csv ; for f in all_seq.csv ; do tail -n +2 ${f} >> scores.csv ; done\")\n",
    "assert os.path.exists(\"scores.csv\"), \"Could not combine scorefiles\"\n",
    "\n",
    "### Calculating the RMSDs of AF2 predictions relative to the diffusion outputs\n",
    "### Catalytic residue sidechain RMSDs are calculated in the reference PDB has REMARK 666 line present\n",
    "\n",
    "analysis_cmd = f\"{PYTHON['general']} {SCRIPT_DIR}/scripts/utils/analyze_af2.py --scorefile scores.csv \"\\\n",
    "               f\"--ref_path {DIFFUSION_DIR}/filtered_structures/ --mpnn --params {' '.join(params)}\"\n",
    "\n",
    "## Analyzing locally\n",
    "p = subprocess.Popen(analysis_cmd, shell=True)\n",
    "(output, err) = p.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c18f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_af2 = pd.read_csv(\"scores.sc\", sep=r\"\\s+\", header=0)\n",
    "\n",
    "### Filtering AF2 scores based on lddt and rmsd\n",
    "# Define your desired cutoffs here:\n",
    "AF2_filters = {\"lDDT\": [80.0, \">=\"],\n",
    "               \"rmsd\": [3, \"<=\"]}  # 1st catalytic residue sc-rmsd\n",
    "\n",
    "scores_af2_filtered = utils.filter_scores(scores_af2, AF2_filters)\n",
    "utils.dump_scorefile(scores_af2_filtered, \"filtered_scores.sc\")\n",
    "\n",
    "## Plotting AF2 scores\n",
    "plt.figure(figsize=(12, 3))\n",
    "for i,k in enumerate(AF2_filters):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.hist(scores_af2[k])\n",
    "    plt.title(k)\n",
    "    plt.xlabel(k)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "    \n",
    "utils.plot_score_pairs(scores_af2, \"lDDT\", \"rmsd\", AF2_filters[\"lDDT\"][0], AF2_filters[\"rmsd\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeb0952",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Copying good predictions to a separate directory\n",
    "os.chdir(AF2_DIR)\n",
    "\n",
    "if len(scores_af2_filtered) > 0:\n",
    "    os.makedirs(\"good\", exist_ok=True)\n",
    "    good_af2_models = [row[\"Output_PDB\"]+\".pdb\" for idx,row in scores_af2_filtered.iterrows()]\n",
    "    for pdb in good_af2_models:\n",
    "        copy2(pdb, f\"good/{pdb}\")\n",
    "    good_af2_models = glob.glob(f\"{AF2_DIR}/good/*.pdb\")\n",
    "else:\n",
    "    sys.exit(\"No good models to continue this pipeline with\")\n",
    "\n",
    "os.chdir(f\"{AF2_DIR}/good\")\n",
    "\n",
    "\n",
    "### Aligning the ligand back into the AF2 predictions.\n",
    "### This is done by aligning the AF2 model to diffusion output and copying over the ligand using PyRosetta.\n",
    "### --fix_catres option will readjust the rotamer and tautomer of \n",
    "### any catalytic residue to be the same as in the reference model.\n",
    "\n",
    "align_cmd = f\"{PYTHON['general']} {SCRIPT_DIR}/scripts/utils/place_ligand_after_af2.py \"\\\n",
    "            f\"--outdir with_{LIGAND} --params {' '.join(params)} --fix_catres \"\\\n",
    "            f\"--pdb {' '.join(good_af2_models)} \"\\\n",
    "            f\"--ref {' '.join(glob.glob(DIFFUSION_DIR+'/filtered_structures/*.pdb'))}\"\n",
    "\n",
    "p = subprocess.Popen(align_cmd, shell=True)\n",
    "(output, err) = p.communicate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac54ca5",
   "metadata": {},
   "source": [
    "## 3.1: Performing binding site design with ligandMPNN / FastRelax\n",
    "(see 3.2 down below for design with [Rosetta FastDesign](#Rosetta_design))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e730b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setting up design directory and commands\n",
    "os.chdir(WDIR)\n",
    "DESIGN_DIR_ligMPNN = f\"{WDIR}/3.1_design_pocket_ligandMPNN\"\n",
    "os.makedirs(DESIGN_DIR_ligMPNN, exist_ok=True)\n",
    "os.chdir(DESIGN_DIR_ligMPNN)\n",
    "\n",
    "AF2_DIR = f\"{WDIR}/2_af2\"\n",
    "os.makedirs(DESIGN_DIR_ligMPNN+\"/logs\", exist_ok=True)\n",
    "\n",
    "### Performing 5 design iterations on each input structure\n",
    "NSTRUCT = 5\n",
    "\n",
    "commands_design = []\n",
    "cmds_filename_des = \"commands_design\"\n",
    "with open(cmds_filename_des, \"w\") as file:\n",
    "    for pdb in glob.glob(f\"{AF2_DIR}/good/with_{LIGAND}/*.pdb\"):\n",
    "        commands_design.append(f\"{PYTHON['general']} {SCRIPT_DIR}/scripts/design/heme_pocket_ligMPNN.py \"\n",
    "                             f\"--pdb {pdb} --nstruct {NSTRUCT} --design_full \"\n",
    "                             f\"--scoring {SCRIPT_DIR}/scripts/design/scoring/heme_scoring.py \"\n",
    "                             f\"--params {' '.join(params)} > logs/{os.path.basename(pdb).replace('.pdb', '.log')}\\n\")\n",
    "        file.write(commands_design[-1])\n",
    "\n",
    "print(\"Example design command:\")\n",
    "print(commands_design[-1])\n",
    "\n",
    "with open(cmds_filename_des) as file:\n",
    "    for line in file:\n",
    "        p = subprocess.Popen(line, shell=True)\n",
    "        (output, err) = p.communicate()\n",
    "\n",
    "### Running design jobs with Slurm.\n",
    "# submit_script = \"submit_design.sh\"\n",
    "# utils.create_slurm_submit_script(filename=submit_script, name=\"3.1_design_pocket_ligMPNN\", mem=\"4g\", \n",
    "#                                  N_cores=1, time=\"3:00:00\", email=EMAIL, array=len(commands_design),\n",
    "#                                  array_commandfile=cmds_filename_des)\n",
    "\n",
    "# if not os.path.exists(DESIGN_DIR_ligMPNN+\"/.done\"):\n",
    "#     p = subprocess.Popen(['sbatch', submit_script], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "#     (output, err) = p.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c2821f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you're done with design and happy with the outputs then mark it as done\n",
    "DESIGN_DIR_ligMPNN = f\"{WDIR}/3.1_design_pocket_ligandMPNN\"\n",
    "os.chdir(DESIGN_DIR_ligMPNN)\n",
    "\n",
    "if not os.path.exists(DESIGN_DIR_ligMPNN+\"/.done\"):\n",
    "    with open(f\"{DESIGN_DIR_ligMPNN}/.done\", \"w\") as file:\n",
    "        file.write(f\"Run user: {username}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0d7f43",
   "metadata": {},
   "source": [
    "### Analyzing ligMPNN / FastRelax design results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2364e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analyzing ligMPNN / FastRelax designs\n",
    "if not os.path.exists(\"scorefile.txt\"):\n",
    "    sys.exit(\"Design job failed, or no successful outputs were produced.\")\n",
    "\n",
    "scores = pd.read_csv(\"scorefile.txt\", sep=\"\\s+\", header=0)\n",
    "\n",
    "filters = {\n",
    " 'nlr_totrms': [1.0, '<='],\n",
    " 'L_SASA': [0.2, '<='],\n",
    " 'score_per_res': [0.0, '<='],\n",
    " 'corrected_ddg': [-2.0, '<='],\n",
    " 'cms_per_atom': [4, '>=']}\n",
    "\n",
    "filtered_scores = utils.filter_scores(scores, filters)\n",
    "\n",
    "## Plotting AF2 scores\n",
    "plt.figure(figsize=(12, 9))\n",
    "for i,k in enumerate(filters):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.hist(scores[k])\n",
    "    plt.title(k)\n",
    "    plt.xlabel(k)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "### Copying good designs over to a new directory\n",
    "if len(filtered_scores) > 0:\n",
    "    os.makedirs(f\"{DESIGN_DIR_ligMPNN}/good\")\n",
    "    for idx, row in filtered_scores.iterrows():\n",
    "        copy2(row[\"description\"]+\".pdb\", \"good/\"+row[\"description\"]+\".pdb\")\n",
    "else:\n",
    "    print(\"No good designs created, bummer...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c82f60",
   "metadata": {},
   "source": [
    "## 4.1 Performing ligandMPNN redesign on the 2nd layer residues\n",
    "\n",
    "Resampling residues that are not in the pocket, but also not very far from the pocket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f470af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(glob.glob(DESIGN_DIR_ligMPNN+'/good/*.pdb')) == 0:\n",
    "    sys.exit(\"No designs to run 2nd MPNN on.\")\n",
    "\n",
    "os.chdir(WDIR)\n",
    "DESIGN_DIR_2nd_mpnn = f\"{WDIR}/4.1_2nd_mpnn\"\n",
    "os.makedirs(DESIGN_DIR_2nd_mpnn, exist_ok=True)\n",
    "os.chdir(DESIGN_DIR_2nd_mpnn)\n",
    "\n",
    "### Making a JSON file specifiying designable positions for each structure.\n",
    "### Will also make non-pocket ALA positions as designable.\n",
    "### This is to fix any surface ALA-patches that previous MPNN may have introduced.\n",
    "\n",
    "make_json_cmd = f\"{PYTHON['general']} {SCRIPT_DIR}/scripts/design/setup_ligand_mpnn_2nd_layer.py \"\\\n",
    "                f\"--params {' '.join(params)} --ligand {LIGAND} --output_path parsed_pdbs_lig.jsonl \"\\\n",
    "                 \"--output_path masked_pos.jsonl --dist_bb 6.0 --dist_sc 5.0 \"\\\n",
    "                f\"--pdb {' '.join(glob.glob(DESIGN_DIR_ligMPNN+'/good/*.pdb'))}\"\n",
    "\n",
    "p = subprocess.Popen(make_json_cmd, shell=True)\n",
    "(output, err) = p.communicate()\n",
    "\n",
    "\n",
    "### Setting up ligandMPNN run commands\n",
    "## We're doing design with 2 temperatures (more conservative than before), and 5 sequences each.\n",
    "\n",
    "MPNN_temperatures = [0.1, 0.2]\n",
    "MPNN_outputs_per_temperature = 5\n",
    "MPNN_omit_AAs = \"CM\"\n",
    "\n",
    "commands_mpnn = []\n",
    "cmds_filename_mpnn = \"commands_mpnn\"\n",
    "with open(cmds_filename_mpnn, \"w\") as file:\n",
    "    for T in MPNN_temperatures:\n",
    "        for f in glob.glob(DESIGN_DIR_ligMPNN+'/good/*.pdb'):\n",
    "            commands_mpnn.append(f\"{PYTHON['proteinMPNN']} {proteinMPNN_script} \"\n",
    "                                 f\"--model_type ligand_mpnn --ligand_mpnn_use_atom_context 1 \"\n",
    "                                 \"--fixed_residues_multi masked_pos.jsonl --out_folder ./ \"\n",
    "                                 f\"--number_of_batches {MPNN_outputs_per_temperature} --temperature {T} \"\n",
    "                                 f\"--omit_AA {MPNN_omit_AAs} --pdb_path {f} \"\n",
    "                                 f\"--checkpoint_ligand_mpnn {SCRIPT_DIR}/lib/LigandMPNN/model_params/ligandmpnn_v_32_010_25.pt\\n\")\n",
    "            file.write(commands_mpnn[-1])\n",
    "\n",
    "print(\"Example MPNN command:\")\n",
    "print(commands_mpnn[-1])\n",
    "\n",
    "### Running ligandMPNN with Slurm.\n",
    "### Grouping jobs with 10 commands per one array job.\n",
    "\n",
    "with open(cmds_filename_mpnn) as file:\n",
    "    for line in file:\n",
    "        p = subprocess.Popen(line, shell=True)p\n",
    "        (output, err) = p.communicate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60859c4-acfd-4fa7-a120-c28bde11a028",
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you're done with design and happy with the outputs then mark it as done\n",
    "DESIGN_DIR_2nd_mpnn = f\"{WDIR}/4.1_2nd_mpnn\"\n",
    "os.chdir(DESIGN_DIR_2nd_mpnn)\n",
    "\n",
    "if not os.path.exists(DESIGN_DIR_2nd_mpnn+\"/.done\"):\n",
    "    with open(f\"{DESIGN_DIR_2nd_mpnn}/.done\", \"w\") as file:\n",
    "        file.write(f\"Run user: {username}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd97aea",
   "metadata": {},
   "source": [
    "## 5.1 AlphaFold2 predictions on the 2nd MPNN run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e96fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(WDIR)\n",
    "DESIGN_DIR_2nd_mpnn = f\"{WDIR}/4.1_2nd_mpnn\"\n",
    "assert os.path.exists(DESIGN_DIR_2nd_mpnn+\"/.done\"), \"2nd MPNN has not been performed!\"\n",
    "\n",
    "AF2_DIR = f\"{WDIR}/5.1_2nd_af2\"\n",
    "os.makedirs(AF2_DIR, exist_ok=True)\n",
    "os.chdir(AF2_DIR)\n",
    "\n",
    "### First collecting MPNN outputs and creating FASTA files for AF2 input\n",
    "mpnn_fasta = utils.parse_fasta_files(glob.glob(f\"{DESIGN_DIR_2nd_mpnn}/seqs/*.fa\"))\n",
    "# Giving sequences unique names based on input PDB name, temperature, and sequence identifier\n",
    "_mpnn_fasta = {}\n",
    "for k, seq in mpnn_fasta.items():\n",
    "    if \"model_path\" in k:\n",
    "        _mpnn_fasta[k.split(\",\")[0]+\"_native\"] = seq.strip()\n",
    "    else:\n",
    "        _mpnn_fasta[k.split(\",\")[0]+\"_\"+k.split(\",\")[2].replace(\" T=\", \"T\")+\"_0_\"+k.split(\",\")[1].replace(\" id=\", \"\")] = seq.strip()\n",
    "mpnn_fasta = {k:v for k,v in _mpnn_fasta.items()}\n",
    "\n",
    "print(f\"A total on {len(mpnn_fasta)} sequences will be predicted.\")\n",
    "\n",
    "## Splitting the MPNN sequences based on length\n",
    "## and grouping them in smaller batches for each AF2 job\n",
    "## Use group size of >40 when running on GPU. Also depends on how many sequences and resources you have.\n",
    "SEQUENCES_PER_AF2_JOB = 5  # CPU\n",
    "if USE_GPU_for_AF2 is True:\n",
    "    SEQUENCES_PER_AF2_JOB = 100  # GPU\n",
    "\n",
    "with open('all_seq2.fasta', 'w') as file:\n",
    "    for k in mpnn_fasta:\n",
    "        file.write(f\"{k}\\n{mpnn_fasta[k]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d082ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting up AlphaFold2 run\n",
    "\n",
    "AF2_recycles = 3\n",
    "AF2_models = \"4\"  # add other models to this string if needed\n",
    "\n",
    "commands_af2 = []\n",
    "cmds_filename_af2 = \"commands_af2\"\n",
    "with open(cmds_filename_af2, \"w\") as file:\n",
    "    for ff in glob.glob(\"*.fasta\"):\n",
    "        commands_af2.append(f\"{PYTHON['af2']} {AF2_script} \"\n",
    "                             f\"--af-nrecycles {AF2_recycles} --af-models {AF2_models} \"\n",
    "                             f\"--fasta {ff} --scorefile {ff.replace('.fasta', '.csv')}\\n\")\n",
    "        file.write(commands_af2[-1])\n",
    "\n",
    "print(\"Example AF2 command:\")\n",
    "print(commands_af2[-1])\n",
    "\n",
    "\n",
    "with open(cmds_filename_af2, 'r') as file:\n",
    "    for line in file:\n",
    "        p = subprocess.Popen(line, shell=True)\n",
    "        (output, err) = p.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98095a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you're done with AF2 and happy with the outputs then mark it as done\n",
    "AF2_DIR = f\"{WDIR}/5.1_2nd_af2\"\n",
    "os.chdir(AF2_DIR)\n",
    "DESIGN_DIR_ligMPNN = f\"{WDIR}/3.1_design_pocket_ligandMPNN\"\n",
    "\n",
    "if not os.path.exists(AF2_DIR+\"/.done\"):\n",
    "    with open(f\"{AF2_DIR}/.done\", \"w\") as file:\n",
    "        file.write(f\"Run user: {username}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c770263c",
   "metadata": {},
   "source": [
    "### Analyzing AlphaFold2 predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fd05f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining all CSV scorefiles into one\n",
    "os.system(\"head -n 1 $(ls all_seq2.csv | shuf -n 1) > scores.csv ; for f in all_seq2.csv ; do tail -n +2 ${f} >> scores.csv ; done\")\n",
    "assert os.path.exists(\"scores.csv\"), \"Could not combine scorefiles\"\n",
    "\n",
    "### Calculating the RMSDs of AF2 predictions relative to the diffusion outputs\n",
    "### Catalytic residue sidechain RMSDs are calculated in the reference PDB has REMARK 666 line present\n",
    "\n",
    "analysis_cmd = f\"{PYTHON['general']} {SCRIPT_DIR}/scripts/utils/analyze_af2.py --scorefile scores.csv \"\\\n",
    "               f\"--ref_path {DESIGN_DIR_ligMPNN}/good/ --mpnn --params {' '.join(params)}\"\n",
    "\n",
    "p = subprocess.Popen(analysis_cmd, shell=True)\n",
    "(output, err) = p.communicate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8512c33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualizing and filtering AF2 results\n",
    "AF2_DIR = f\"{WDIR}/5.1_2nd_af2\"\n",
    "os.chdir(AF2_DIR)\n",
    "\n",
    "scores_af2 = pd.read_csv(\"scores.sc\", sep=\"\\s+\", header=0)\n",
    "\n",
    "### Filtering AF2 scores based on lddt and rmsd\n",
    "# Define your desired cutoffs here:\n",
    "AF2_filters = {\"lDDT\": [85.0, \">=\"],\n",
    "               \"rmsd\": [2.5, \"<=\"]}  # 1st catalytic residue sc-rmsd\n",
    "\n",
    "scores_af2_filtered = utils.filter_scores(scores_af2, AF2_filters)\n",
    "utils.dump_scorefile(scores_af2_filtered, \"filtered_scores.sc\")\n",
    "\n",
    "## Plotting AF2 scores\n",
    "plt.figure(figsize=(12, 3))\n",
    "for i,k in enumerate(AF2_filters):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.hist(scores_af2[k])\n",
    "    plt.title(k)\n",
    "    plt.xlabel(k)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "utils.plot_score_pairs(scores_af2, \"lDDT\", \"rmsd\", AF2_filters[\"lDDT\"][0], AF2_filters[\"rmsd\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20de13ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Copying good predictions to a separate directory\n",
    "os.chdir(AF2_DIR)\n",
    "\n",
    "if len(scores_af2_filtered) > 0:\n",
    "    os.makedirs(\"good\", exist_ok=True)\n",
    "    good_af2_models = [row[\"Output_PDB\"]+\".pdb\" for idx,row in scores_af2_filtered.iterrows()]\n",
    "    for pdb in good_af2_models:\n",
    "        copy2(pdb, f\"good/{pdb}\")\n",
    "    good_af2_models = glob.glob(f\"{AF2_DIR}/good/*.pdb\")\n",
    "else:\n",
    "    sys.exit(\"No good models to continue this pipeline with\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc2ddec",
   "metadata": {},
   "source": [
    "## 6.1: Final FastRelax with the ligand\n",
    "Relaxing good AF2 models together with the ligand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268bceef",
   "metadata": {},
   "outputs": [],
   "source": [
    "AF2_DIR = f\"{WDIR}/5.1_2nd_af2\"\n",
    "DESIGN_DIR_ligMPNN = f\"{WDIR}/3.1_design_pocket_ligandMPNN\"\n",
    "assert len(glob.glob(f\"{AF2_DIR}/good/*.pdb\")) > 0, \"No good AF2 models to relax with\"\n",
    "\n",
    "os.chdir(WDIR)\n",
    "RELAX_DIR = f\"{WDIR}/6.1_final_relax\"\n",
    "os.makedirs(RELAX_DIR, exist_ok=True)\n",
    "os.chdir(RELAX_DIR)\n",
    "\n",
    "os.makedirs(RELAX_DIR+\"/logs\", exist_ok=True)\n",
    "\n",
    "## First matching up the AF2 output filenames of step 5 with pocket design filenames from step 3\n",
    "ref_and_model_pairs = []\n",
    "for r in glob.glob(f\"{DESIGN_DIR_ligMPNN}/good/*.pdb\"):\n",
    "    for pdbfile in glob.glob(f\"{AF2_DIR}/good/*.pdb\"):\n",
    "        if os.path.basename(r).replace(\".pdb\", \"_\") in pdbfile:\n",
    "            ref_and_model_pairs.append((r, pdbfile))\n",
    "\n",
    "assert len(ref_and_model_pairs) == len(glob.glob(f\"{AF2_DIR}/good/*.pdb\")), \"Was not able to match all models with reference structures\"\n",
    "\n",
    "\n",
    "## Generating commands for relax jobs\n",
    "### Performing 1 relax iteration on each input structure\n",
    "NSTRUCT = 1\n",
    "\n",
    "commands_relax = []\n",
    "cmds_filename_rlx = \"commands_relax\"\n",
    "with open(cmds_filename_rlx, \"w\") as file:\n",
    "    for r_m in ref_and_model_pairs:\n",
    "        commands_relax.append(f\"{PYTHON['general']} {SCRIPT_DIR}/scripts/design/align_add_ligand_relax.py \"\n",
    "                              f\"--outdir ./ --ligand {LIGAND} --ref_pdb {r_m[0]} \"\n",
    "                              f\"--pdb {r_m[1]} --nstruct {NSTRUCT} \"\n",
    "                              f\"--params {' '.join(params)} > logs/{os.path.basename(r_m[1]).replace('.pdb', '.log')}\\n\")\n",
    "        file.write(commands_relax[-1])\n",
    "\n",
    "print(\"Example design command:\")\n",
    "print(commands_relax[-1])\n",
    "\n",
    "with open(cmds_filename_rlx, 'r') as file:\n",
    "    for line in file:\n",
    "        p = subprocess.Popen(line, shell=True)\n",
    "        (output, err) = p.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366ce521",
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you're done with final relax and happy with the outputs then mark it as done\n",
    "RELAX_DIR = f\"{WDIR}/6.1_final_relax\"\n",
    "os.chdir(RELAX_DIR)\n",
    "\n",
    "if not os.path.exists(RELAX_DIR+\"/.done\"):\n",
    "    with open(f\"{RELAX_DIR}/.done\", \"w\") as file:\n",
    "        file.write(f\"Run user: {username}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d848dbbf",
   "metadata": {},
   "source": [
    "### Analyzing final relaxed structures\n",
    "Filtering them based on the same metrics as was used for the initial design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3744d0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analyzing Rosetta designs\n",
    "RELAX_DIR = f\"{WDIR}/6.1_final_relax\"\n",
    "os.chdir(RELAX_DIR)\n",
    "\n",
    "scores = pd.read_csv(\"scorefile.txt\", sep=r\"\\s+\", header=0)\n",
    "\n",
    "filters = {\n",
    " 'nlr_totrms': [1.0, '<='],\n",
    " 'L_SASA': [0.2, '<='],\n",
    " 'score_per_res': [0.0, '<='],\n",
    " 'corrected_ddg': [-50.0, '<='],\n",
    " 'cms_per_atom': [4.8, '>='],\n",
    " 'rmsd_CA_rlx_in': [1.0, \"<=\"]}  # rmsd_CA_rlx_in is rmsd between relaxed structure and AF2 prediction\n",
    "\n",
    "filtered_scores = utils.filter_scores(scores, filters)\n",
    "\n",
    "## Plotting relax scores\n",
    "plt.figure(figsize=(12, 9))\n",
    "for i,k in enumerate(filters):\n",
    "    if k not in scores.keys():\n",
    "        continue\n",
    "    plt.subplot(4, 3, i+1)\n",
    "    plt.hist(scores[k])\n",
    "    plt.title(k)\n",
    "    plt.xlabel(k)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "### Copying good designs over to a new directory\n",
    "if len(filtered_scores) > 0:\n",
    "    os.makedirs(f\"{RELAX_DIR}/good\", exist_ok=True)\n",
    "    for idx, row in filtered_scores.iterrows():\n",
    "        copy2(row[\"description\"]+\".pdb\", \"good/\"+row[\"description\"]+\".pdb\")\n",
    "else:\n",
    "    print(\"No good designs created, bummer...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e448ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(filtered_scores) > 0:\n",
    "    print(f\"CONGRATULATIONS! You have successfully designed {len(filtered_scores)} proteins against ligand {LIGAND}\")\n",
    "    print(\"You can find the design models in the directory:\\n\"\n",
    "          f\"    {RELAX_DIR}/good\")\n",
    "    print(\"\\nIt is advised you manually inspect them before ordering.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def0617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPAINT_DIR = f\"{WDIR}/inpainting/0_inpainting\"\n",
    "assert len(glob.glob(INPAINT_DIR+\"/alignment/*.pdb\")) > 0, \"No good backbones found!\"\n",
    "\n",
    "os.chdir(WDIR)\n",
    "\n",
    "MPNN_DIR = f\"{WDIR}/inpainting/1_proteinmpnn\"\n",
    "os.makedirs(MPNN_DIR, exist_ok=True)\n",
    "os.chdir(MPNN_DIR)\n",
    "\n",
    "### Parsing diffusion output TRB files to extract fixed motif residues\n",
    "## These residues will not be redesigned with proteinMPNN\n",
    "mask_json_cmd = f\"{PYTHON['general']} {SCRIPT_DIR}/scripts/design/make_maskdict_from_trb.py \"\\\n",
    "                f\"--out masked_pos.jsonl --trb {' '.join(glob.glob(INPAINT_DIR+'/*.trb'))}\"\n",
    "p = subprocess.Popen(mask_json_cmd, shell=True)\n",
    "(output, err) = p.communicate()\n",
    "\n",
    "assert os.path.exists(\"masked_pos.jsonl\"), \"Failed to create masked positions JSONL file\"\n",
    "\n",
    "\n",
    "### Setting up proteinMPNN run commands\n",
    "## We're doing design with 3 temperatures, and 5 sequences each.\n",
    "## This usually gives decent success with designable backbones. For more complicated cases consider doing >100 sequences.\n",
    "\n",
    "MPNN_temperatures = [0.1, 0.2, 0.3]\n",
    "MPNN_outputs_per_temperature = 5\n",
    "MPNN_omit_AAs = \"CM\"\n",
    "\n",
    "commands_mpnn = []\n",
    "cmds_filename_mpnn = \"commands_mpnn\"\n",
    "with open(cmds_filename_mpnn, \"w\") as file:\n",
    "    for T in MPNN_temperatures:\n",
    "        for f in glob.glob(INPAINT_DIR+\"/alignment/*.pdb\"):\n",
    "            commands_mpnn.append(f\"{PYTHON['proteinMPNN']} {proteinMPNN_script} \"\n",
    "                                 f\"--model_type protein_mpnn --ligand_mpnn_use_atom_context 0 \"\n",
    "                                 \"--fixed_residues_multi masked_pos.jsonl --out_folder ./ \"\n",
    "                                 f\"--number_of_batches {MPNN_outputs_per_temperature} --temperature {T} \"\n",
    "                                 f\"--omit_AA {MPNN_omit_AAs} --pdb_path {f} \"\n",
    "                                 f\"--checkpoint_protein_mpnn {SCRIPT_DIR}/lib/LigandMPNN/model_params/proteinmpnn_v_48_020.pt\\n\")\n",
    "            file.write(commands_mpnn[-1])\n",
    "\n",
    "            \n",
    "            \n",
    "print(f\"{len(commands_mpnn)} MPNN jobs to run\")\n",
    "print(\"Example MPNN command:\")\n",
    "print(commands_mpnn[-1])\n",
    "\n",
    "### Running proteinMPNN with Slurm.\n",
    "### Grouping jobs with 10 commands per one array job.\n",
    "\n",
    "submit_script = \"submit_mpnn.sh\"\n",
    "utils.create_slurm_submit_script(filename=submit_script, name=\"1_proteinmpnn\", mem=\"4g\", \n",
    "                                 N_cores=1, time=\"0:15:00\", email=EMAIL, array=len(commands_mpnn),\n",
    "                                 array_commandfile=cmds_filename_mpnn, group=10)\n",
    "\n",
    "if not os.path.exists(MPNN_DIR+\"/.done\"):\n",
    "    p = subprocess.Popen(['sbatch', submit_script], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    (output, err) = p.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57ab8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you're done with MPNN and happy with the outputs then mark it as done\n",
    "MPNN_DIR = f\"{WDIR}/inpainting/1_proteinmpnn\"\n",
    "os.chdir(MPNN_DIR)\n",
    "\n",
    "if not os.path.exists(MPNN_DIR+\"/.done\"):\n",
    "    with open(f\"{MPNN_DIR}/.done\", \"w\") as file:\n",
    "        file.write(f\"Run user: {username}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f19d9ce",
   "metadata": {},
   "source": [
    "## 2: Running AlphaFold2\n",
    "Performing AF2 single sequence predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8e267d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(WDIR)\n",
    "assert len(glob.glob(MPNN_DIR+\"/seqs/*.fa\")) > 0, \"No MPNN outputs to run AF2 on\"\n",
    "\n",
    "AF2_DIR = f\"{WDIR}/inpainting/2_af2\"\n",
    "os.makedirs(AF2_DIR, exist_ok=True)\n",
    "os.chdir(AF2_DIR)\n",
    "\n",
    "### First collecting MPNN outputs and creating FASTA files for AF2 input\n",
    "mpnn_fasta = utils.parse_fasta_files(glob.glob(f\"{MPNN_DIR}/seqs/*.fa\"))\n",
    "mpnn_fasta = {k: seq.strip() for k, seq in mpnn_fasta.items() if \"model_path\" not in k}  # excluding the diffused poly-A sequence\n",
    "# Giving sequences unique names based on input PDB name, temperature, and sequence identifier\n",
    "mpnn_fasta = {k.split(\",\")[0]+\"_\"+k.split(\",\")[2].replace(\" T=\", \"T\")+\"_0_\"+k.split(\",\")[1].replace(\" id=\", \"\"): seq for k, seq in mpnn_fasta.items()}\n",
    "\n",
    "print(f\"A total on {len(mpnn_fasta)} sequences will be predicted.\")\n",
    "\n",
    "## Splitting the MPNN sequences based on length\n",
    "## and grouping them in smaller batches for each AF2 job\n",
    "## Use group size of >40 when running on GPU. Also depends on how many sequences and resources you have.\n",
    "SEQUENCES_PER_AF2_JOB = 4  # CPU\n",
    "if USE_GPU_for_AF2 is True:\n",
    "    SEQUENCES_PER_AF2_JOB = 100  # GPU\n",
    "mpnn_fasta_split = utils.split_fasta_based_on_length(mpnn_fasta, SEQUENCES_PER_AF2_JOB, write_files=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411d5b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting up AlphaFold2 run\n",
    "\n",
    "AF2_recycles = 3\n",
    "AF2_models = \"4\"  # add other models to this string if needed, i.e. \"3 4 5\"\n",
    "\n",
    "commands_af2 = []\n",
    "cmds_filename_af2 = \"commands_af2\"\n",
    "with open(cmds_filename_af2, \"w\") as file:\n",
    "    for ff in glob.glob(\"*.fasta\"):\n",
    "        commands_af2.append(f\"{PYTHON['af2']} {AF2_script} \"\n",
    "                             f\"--af-nrecycles {AF2_recycles} --af-models {AF2_models} \"\n",
    "                             f\"--fasta {ff} --scorefile {ff.replace('.fasta', '.csv')}\\n\")\n",
    "        file.write(commands_af2[-1])\n",
    "\n",
    "print(\"Example AF2 command:\")\n",
    "print(commands_af2[-1])\n",
    "\n",
    "### Running AF2 with Slurm.\n",
    "### Running jobs on the CPU. It takes ~10 minutes per sequence\n",
    "### \n",
    "\n",
    "submit_script = \"submit_af2.sh\"\n",
    "if USE_GPU_for_AF2 is True:\n",
    "    utils.create_slurm_submit_script(filename=submit_script, name=\"2_af2\", mem=\"6g\", \n",
    "                                     N_cores=2, gpu=True, gres=\"gpu:rtx2080:1\", time=\"00:12:00\", email=EMAIL, array=len(commands_af2),\n",
    "                                     array_commandfile=cmds_filename_af2)\n",
    "else:\n",
    "    utils.create_slurm_submit_script(filename=submit_script, name=\"2_af2\", mem=\"6g\", \n",
    "                                     N_cores=4, time=\"01:00:00\", email=EMAIL, array=len(commands_af2),\n",
    "                                     array_commandfile=cmds_filename_af2)\n",
    "\n",
    "if not os.path.exists(AF2_DIR+\"/.done\"):\n",
    "    p = subprocess.Popen(['sbatch', submit_script], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    (output, err) = p.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0c7b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you're done with AF2 and happy with the outputs then mark it as done\n",
    "AF2_DIR = f\"{WDIR}/inpainting/2_af2\"\n",
    "os.chdir(AF2_DIR)\n",
    "\n",
    "if not os.path.exists(AF2_DIR+\"/.done\"):\n",
    "    with open(f\"{AF2_DIR}/.done\", \"w\") as file:\n",
    "        file.write(f\"Run user: {username}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184907b7",
   "metadata": {},
   "source": [
    "### Analyzing AlphaFold2 predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c85aeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPAINT_DIR = f\"{WDIR}/inpainting/0_inpainting\"\n",
    "\n",
    "# Combining all CSV scorefiles into one\n",
    "os.system(\"head -n 1 $(ls *aa*.csv | shuf -n 1) > scores.csv ; for f in *aa*.csv ; do tail -n +2 ${f} >> scores.csv ; done\")\n",
    "assert os.path.exists(\"scores.csv\"), \"Could not combine scorefiles\"\n",
    "\n",
    "### Calculating the RMSDs of AF2 predictions relative to the diffusion outputs\n",
    "### Catalytic residue sidechain RMSDs are calculated in the reference PDB has REMARK 666 line present\n",
    "\n",
    "analysis_cmd = f\"{PYTHON['general']} {SCRIPT_DIR}/scripts/utils/analyze_af2.py --scorefile scores.csv \"\\\n",
    "               f\"--ref_path {INPAINT_DIR}/alignment/ --mpnn --params {' '.join(params)}\"\n",
    "\n",
    "p = subprocess.Popen(analysis_cmd, shell=True)\n",
    "(output, err) = p.communicate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146e5a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualizing and filtering AF2 predictions\n",
    "AF2_DIR = f\"{WDIR}/inpainting/2_af2\"\n",
    "\n",
    "scores_af2 = pd.read_csv(\"scores.sc\", sep=\"\\s+\", header=0)\n",
    "\n",
    "### Filtering AF2 scores based on lddt and rmsd\n",
    "# Define your desired cutoffs here:\n",
    "AF2_filters = {\"lDDT\": [85.0, \">=\"],\n",
    "               \"rmsd\": [1.5, \"<=\"],\n",
    "               \"rmsd_SR1\": [2.0, \"<=\"]}  # 1st catalytic residue sc-rmsd\n",
    "\n",
    "scores_af2_filtered = utils.filter_scores(scores_af2, AF2_filters)\n",
    "utils.dump_scorefile(scores_af2_filtered, \"filtered_scores.sc\")\n",
    "\n",
    "## Plotting AF2 scores\n",
    "plt.figure(figsize=(12, 3))\n",
    "for i,k in enumerate(AF2_filters):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.hist(scores_af2[k])\n",
    "    plt.title(k)\n",
    "    plt.xlabel(k)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "    \n",
    "utils.plot_score_pairs(scores_af2, \"lDDT\", \"rmsd\", AF2_filters[\"lDDT\"][0], AF2_filters[\"rmsd\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ac5be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Copying good predictions to a separate directory\n",
    "os.chdir(AF2_DIR)\n",
    "INPAINT_DIR = f\"{WDIR}/inpainting/0_inpainting\"\n",
    "\n",
    "if len(scores_af2_filtered) > 0:\n",
    "    os.makedirs(\"good\", exist_ok=True)\n",
    "    good_af2_models = [row[\"Output_PDB\"]+\".pdb\" for idx,row in scores_af2_filtered.iterrows()]\n",
    "    for pdb in good_af2_models:\n",
    "        copy2(pdb, f\"good/{pdb}\")\n",
    "    good_af2_models = glob.glob(f\"{AF2_DIR}/good/*.pdb\")\n",
    "else:\n",
    "    sys.exit(\"No good models to continue this pipeline with\")\n",
    "\n",
    "os.chdir(f\"{AF2_DIR}/good\")\n",
    "\n",
    "\n",
    "### Aligning the ligand back into the AF2 predictions.\n",
    "### This is done by aligning the AF2 model to diffusion output and copying over the ligand using PyRosetta.\n",
    "### --fix_catres option will re-adjust the rotamer and tautomer of \n",
    "### any catalytic residue to be the same as in the reference model.\n",
    "\n",
    "align_cmd = f\"{PYTHON['general']} {SCRIPT_DIR}/scripts/utils/place_ligand_after_af2.py \"\\\n",
    "            f\"--outdir with_heme --params {' '.join(params)} --fix_catres \"\\\n",
    "            f\"--pdb {' '.join(good_af2_models)} \"\\\n",
    "            f\"--ref {' '.join(glob.glob(INPAINT_DIR+'/alignment/*.pdb'))}\"\n",
    "\n",
    "p = subprocess.Popen(align_cmd, shell=True)\n",
    "(output, err) = p.communicate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9ad826",
   "metadata": {},
   "source": [
    "## 3.1: Performing binding site design with LigandMPNN / FastRelax\n",
    "<a id='ligmpnn_fr'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ad0fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setting up design directory and commands\n",
    "os.chdir(WDIR)\n",
    "DESIGN_DIR_ligMPNN = f\"{WDIR}/inpainting/3.1_design_pocket_ligandMPNN\"\n",
    "os.makedirs(DESIGN_DIR_ligMPNN, exist_ok=True)\n",
    "os.chdir(DESIGN_DIR_ligMPNN)\n",
    "\n",
    "AF2_DIR = f\"{WDIR}/inpainting/2_af2\"  # change this if you want to run it on inpainting outputs\n",
    "os.makedirs(DESIGN_DIR_ligMPNN+\"/logs\", exist_ok=True)\n",
    "\n",
    "### Performing 5 design iterations on each input structure\n",
    "NSTRUCT = 10\n",
    "cstfile = f\"{SCRIPT_DIR}/theozyme/HBA/HBA_CYS_UPO.cst\"\n",
    "\n",
    "commands_design = []\n",
    "cmds_filename_des = \"commands_design\"\n",
    "with open(cmds_filename_des, \"w\") as file:\n",
    "    for pdb in glob.glob(f\"{AF2_DIR}/good/with_heme/*.pdb\"):\n",
    "        commands_design.append(f\"{PYTHON['general']} {SCRIPT_DIR}/scripts/design/heme_pocket_ligMPNN.py \"\n",
    "                             f\"--pdb {pdb} --nstruct {NSTRUCT} \"\n",
    "                             f\"--scoring {SCRIPT_DIR}/scripts/design/scoring/heme_scoring.py \"\n",
    "                             f\"--params {' '.join(params)} --cstfile {cstfile} > logs/{os.path.basename(pdb).replace('.pdb', '.log')}\\n\")\n",
    "        file.write(commands_design[-1])\n",
    "\n",
    "print(\"Example design command:\")\n",
    "print(commands_design[-1])\n",
    "\n",
    "\n",
    "### Running design jobs with Slurm.\n",
    "submit_script = \"submit_design.sh\"\n",
    "utils.create_slurm_submit_script(filename=submit_script, name=\"3.1_design_pocket_ligMPNN\", mem=\"4g\", \n",
    "                                 N_cores=1, time=\"3:00:00\", email=EMAIL, array=len(commands_design),\n",
    "                                 array_commandfile=cmds_filename_des)\n",
    "\n",
    "if not os.path.exists(DESIGN_DIR_ligMPNN+\"/.done\"):\n",
    "    p = subprocess.Popen(['sbatch', submit_script], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    (output, err) = p.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77afc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "DESIGN_DIR_ligMPNN = f\"{WDIR}/inpainting/3.1_design_pocket_ligandMPNN\"\n",
    "os.chdir(DESIGN_DIR_ligMPNN)\n",
    "\n",
    "## If you're done with design and happy with the outputs then mark it as done\n",
    "if not os.path.exists(DESIGN_DIR_ligMPNN+\"/.done\"):\n",
    "    with open(f\"{DESIGN_DIR_ligMPNN}/.done\", \"w\") as file:\n",
    "        file.write(f\"Run user: {username}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcf6697",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Analyzing Rosetta designs\n",
    "scores = pd.read_csv(\"scorefile.txt\", sep=\"\\s+\", header=0)\n",
    "\n",
    "filters = {'all_cst': [1.0, '<='],\n",
    " 'nlr_SR1_rms': [0.8, '<='],\n",
    " 'nlr_totrms': [1.0, '<='],\n",
    " 'L_SASA': [0.2, '<='],\n",
    " 'COO_hbond': [1.0, '='],\n",
    " 'heme_angle_wrst': [80.0, '>='],\n",
    " 'score_per_res': [0.0, '<='],\n",
    " 'corrected_ddg': [-50.0, '<='],\n",
    " 'cms_per_atom': [4.8, '>=']}\n",
    "\n",
    "filtered_scores = utils.filter_scores(scores, filters)\n",
    "\n",
    "## Plotting design scores\n",
    "plt.figure(figsize=(12, 9))\n",
    "for i,k in enumerate(filters):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.hist(scores[k])\n",
    "    plt.title(k)\n",
    "    plt.xlabel(k)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "### Copying good designs over to a new directory\n",
    "if len(filtered_scores) > 0:\n",
    "    os.makedirs(f\"{DESIGN_DIR_ligMPNN}/good\", exist_ok=True)\n",
    "    for idx, row in filtered_scores.iterrows():\n",
    "        copy2(row[\"description\"]+\".pdb\", \"good/\"+row[\"description\"]+\".pdb\")\n",
    "else:\n",
    "    print(\"No good designs created, bummer...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56c82b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8397b389",
   "metadata": {},
   "source": [
    "## 4.1 Performing ligandMPNN redesign on the 2nd layer residues\n",
    "\n",
    "Resampling residues that are not in the pocket, but also not very far from the pocket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac14eee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if len(glob.glob(DESIGN_DIR_ligMPNN+'/good/*.pdb')) == 0:\n",
    "    sys.exit(\"No designs to run 2nd MPNN on.\")\n",
    "\n",
    "os.chdir(WDIR)\n",
    "DESIGN_DIR_2nd_mpnn = f\"{WDIR}/inpainting/4.1_2nd_mpnn\"\n",
    "os.makedirs(DESIGN_DIR_2nd_mpnn, exist_ok=True)\n",
    "os.chdir(DESIGN_DIR_2nd_mpnn)\n",
    "\n",
    "### Making a JSON file specifiying designable positions for each structure.\n",
    "### Will also make non-pocket ALA positions as designable.\n",
    "### This is to fix any surface ALA-patches that previous MPNN may have introduced.\n",
    "\n",
    "make_json_cmd = f\"{PYTHON['general']} {SCRIPT_DIR}/scripts/design/setup_ligand_mpnn_2nd_layer.py \"\\\n",
    "                f\"--params {' '.join(params)} --ligand {LIGAND} --output_path parsed_pdbs_lig.jsonl \"\\\n",
    "                 \"--output_path masked_pos.jsonl --dist_bb 6.0 --dist_sc 5.0 \"\\\n",
    "                f\"--pdb {' '.join(glob.glob(DESIGN_DIR_ligMPNN+'/good/*.pdb'))}\"\n",
    "\n",
    "p = subprocess.Popen(make_json_cmd, shell=True)\n",
    "(output, err) = p.communicate()\n",
    "\n",
    "if not os.path.exists(\"masked_pos.jsonl\"):\n",
    "    sys.exit()\n",
    "\n",
    "### Setting up ligandMPNN run commands\n",
    "## We're doing design with 2 temperatures (more conservative than before), and 5 sequences each.\n",
    "\n",
    "MPNN_temperatures = [0.1, 0.2]\n",
    "MPNN_outputs_per_temperature = 5\n",
    "MPNN_omit_AAs = \"CM\"\n",
    "\n",
    "commands_mpnn = []\n",
    "cmds_filename_mpnn = \"commands_mpnn\"\n",
    "with open(cmds_filename_mpnn, \"w\") as file:\n",
    "    for T in MPNN_temperatures:\n",
    "        for f in glob.glob(DESIGN_DIR_ligMPNN+'/good/*.pdb'):\n",
    "            commands_mpnn.append(f\"{PYTHON['proteinMPNN']} {proteinMPNN_script} \"\n",
    "                                 f\"--model_type ligand_mpnn --ligand_mpnn_use_atom_context 1 \"\n",
    "                                 \"--fixed_residues_multi masked_pos.jsonl --out_folder ./ \"\n",
    "                                 f\"--number_of_batches {MPNN_outputs_per_temperature} --temperature {T} \"\n",
    "                                 f\"--omit_AA {MPNN_omit_AAs} --pdb_path {f} \"\n",
    "                                 f\"--checkpoint_protein_mpnn {SCRIPT_DIR}/lib/LigandMPNN/model_params/ligandmpnn_v_32_010_25.pt\\n\")\n",
    "            file.write(commands_mpnn[-1])\n",
    "\n",
    "print(\"Example MPNN command:\")\n",
    "print(commands_mpnn[-1])\n",
    "\n",
    "### Running ligandMPNN with Slurm.\n",
    "### Grouping jobs with 10 commands per one array job.\n",
    "\n",
    "submit_script = \"submit_mpnn.sh\"\n",
    "utils.create_slurm_submit_script(filename=submit_script, name=\"4.1_2nd_mpnn\", mem=\"8g\", \n",
    "                                 N_cores=1, time=\"0:15:00\", email=EMAIL, array=len(commands_mpnn),\n",
    "                                 array_commandfile=cmds_filename_mpnn, group=10)\n",
    "\n",
    "if not os.path.exists(DESIGN_DIR_2nd_mpnn+\"/.done\"):\n",
    "    p = subprocess.Popen(['sbatch', submit_script], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    (output, err) = p.communicate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0233f278",
   "metadata": {},
   "outputs": [],
   "source": [
    "DESIGN_DIR_ligMPNN = f\"{WDIR}/inpainting/3.1_design_pocket_ligandMPNN\"\n",
    "DESIGN_DIR_2nd_mpnn = f\"{WDIR}/inpainting/4.1_2nd_mpnn\"\n",
    "os.chdir(DESIGN_DIR_2nd_mpnn)\n",
    "if len(glob.glob(DESIGN_DIR_ligMPNN+'/good/*.pdb')) == 0:\n",
    "    sys.exit(\"No designs to run 2nd MPNN on.\")\n",
    "\n",
    "## If you're done with design and happy with the outputs then mark it as done\n",
    "if not os.path.exists(DESIGN_DIR_2nd_mpnn+\"/.done\"):\n",
    "    with open(f\"{DESIGN_DIR_2nd_mpnn}/.done\", \"w\") as file:\n",
    "        file.write(f\"Run user: {username}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d495bf96",
   "metadata": {},
   "source": [
    "## 5.1 AlphaFold2 predictions on the 2nd MPNN run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cba4381",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(WDIR)\n",
    "assert os.path.exists(DESIGN_DIR_2nd_mpnn+\"/.done\"), \"2nd MPNN has not been performed!\"\n",
    "\n",
    "AF2_DIR = f\"{WDIR}/inpainting/5.1_2nd_af2\"\n",
    "os.makedirs(AF2_DIR, exist_ok=True)\n",
    "os.chdir(AF2_DIR)\n",
    "\n",
    "### First collecting MPNN outputs and creating FASTA files for AF2 input\n",
    "mpnn_fasta = utils.parse_fasta_files(glob.glob(f\"{DESIGN_DIR_2nd_mpnn}/seqs/*.fa\"))\n",
    "# Giving sequences unique names based on input PDB name, temperature, and sequence identifier\n",
    "_mpnn_fasta = {}\n",
    "for k, seq in mpnn_fasta.items():\n",
    "    if \"model_path\" in k:\n",
    "        _mpnn_fasta[k.split(\",\")[0]+\"_native\"] = seq.strip()\n",
    "    else:\n",
    "        _mpnn_fasta[k.split(\",\")[0]+\"_\"+k.split(\",\")[2].replace(\" T=\", \"T\")+\"_0_\"+k.split(\",\")[1].replace(\" id=\", \"\")] = seq.strip()\n",
    "mpnn_fasta = {k:v for k,v in _mpnn_fasta.items()}\n",
    "\n",
    "print(f\"A total on {len(mpnn_fasta)} sequences will be predicted.\")\n",
    "\n",
    "## Splitting the MPNN sequences based on length\n",
    "## and grouping them in smaller batches for each AF2 job\n",
    "## Use group size of >40 when running on GPU. Also depends on how many sequences and resources you have.\n",
    "SEQUENCES_PER_AF2_JOB = 5  # CPU\n",
    "if USE_GPU_for_AF2 is True:\n",
    "    SEQUENCES_PER_AF2_JOB = 100  # GPU\n",
    "mpnn_fasta_split = utils.split_fasta_based_on_length(mpnn_fasta, SEQUENCES_PER_AF2_JOB, write_files=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b309a47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting up AlphaFold2 run\n",
    "\n",
    "AF2_recycles = 3\n",
    "AF2_models = \"4\"  # add other models to this string if needed\n",
    "\n",
    "commands_af2 = []\n",
    "cmds_filename_af2 = \"commands_af2\"\n",
    "with open(cmds_filename_af2, \"w\") as file:\n",
    "    for ff in glob.glob(\"*.fasta\"):\n",
    "        commands_af2.append(f\"{PYTHON['af2']} {AF2_script} \"\n",
    "                             f\"--af-nrecycles {AF2_recycles} --af-models {AF2_models} \"\n",
    "                             f\"--fasta {ff} --scorefile {ff.replace('.fasta', '.csv')}\\n\")\n",
    "        file.write(commands_af2[-1])\n",
    "\n",
    "print(\"Example AF2 command:\")\n",
    "print(commands_af2[-1])\n",
    "\n",
    "### Running AF2 with Slurm.\n",
    "### Running jobs on the CPU. It takes ~10 minutes per sequence\n",
    "\n",
    "submit_script = \"submit_af2.sh\"\n",
    "if USE_GPU_for_AF2 is True:\n",
    "    utils.create_slurm_submit_script(filename=submit_script, name=\"5.1_2nd_af2\", mem=\"6g\", \n",
    "                                     N_cores=2, gpu=True, gres=\"gpu:rtx2080:1\", time=\"00:12:00\", email=EMAIL, array=len(commands_af2),\n",
    "                                     array_commandfile=cmds_filename_af2)\n",
    "else:\n",
    "    utils.create_slurm_submit_script(filename=submit_script, name=\"5.1_2nd_af2\", mem=\"6g\", \n",
    "                                     N_cores=4, time=\"01:00:00\", email=EMAIL, array=len(commands_af2),\n",
    "                                     array_commandfile=cmds_filename_af2)\n",
    "\n",
    "if not os.path.exists(AF2_DIR+\"/.done\"):\n",
    "    p = subprocess.Popen(['sbatch', submit_script], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    (output, err) = p.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0915fdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you're done with AF2 and happy with the outputs then mark it as done\n",
    "AF2_DIR = f\"{WDIR}/inpainting/5.1_2nd_af2\"\n",
    "DESIGN_DIR_ligMPNN = f\"{WDIR}/inpainting/3.1_design_pocket_ligandMPNN\"\n",
    "os.chdir(AF2_DIR)\n",
    "\n",
    "if not os.path.exists(AF2_DIR+\"/.done\"):\n",
    "    with open(f\"{AF2_DIR}/.done\", \"w\") as file:\n",
    "        file.write(f\"Run user: {username}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d53a010",
   "metadata": {},
   "source": [
    "### Analyzing AlphaFold2 predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1266a22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining all CSV scorefiles into one\n",
    "os.system(\"head -n 1 $(ls *aa*.csv | shuf -n 1) > scores.csv ; for f in *aa*.csv ; do tail -n +2 ${f} >> scores.csv ; done\")\n",
    "assert os.path.exists(\"scores.csv\"), \"Could not combine scorefiles\"\n",
    "\n",
    "### Calculating the RMSDs of AF2 predictions relative to the diffusion outputs\n",
    "### Catalytic residue sidechain RMSDs are calculated in the reference PDB has REMARK 666 line present\n",
    "\n",
    "analysis_cmd = f\"{PYTHON['general']} {SCRIPT_DIR}/scripts/utils/analyze_af2.py --scorefile scores.csv \"\\\n",
    "               f\"--ref_path {DESIGN_DIR_ligMPNN}/good/ --mpnn --params {' '.join(params)}\"\n",
    "\n",
    "p = subprocess.Popen(analysis_cmd, shell=True)\n",
    "(output, err) = p.communicate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981cc7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualizing and filtering AF2 results\n",
    "AF2_DIR = f\"{WDIR}/inpainting/5.1_2nd_af2\"\n",
    "os.chdir(AF2_DIR)\n",
    "\n",
    "scores_af2 = pd.read_csv(\"scores.sc\", sep=\"\\s+\", header=0)\n",
    "\n",
    "### Filtering AF2 scores based on lddt and rmsd\n",
    "# Define your desired cutoffs here:\n",
    "AF2_filters = {\"lDDT\": [85.0, \">=\"],\n",
    "               \"rmsd\": [1.2, \"<=\"],\n",
    "               \"rmsd_SR1\": [1.0, \"<=\"]}  # 1st catalytic residue sc-rmsd\n",
    "\n",
    "scores_af2_filtered = utils.filter_scores(scores_af2, AF2_filters)\n",
    "utils.dump_scorefile(scores_af2_filtered, \"filtered_scores.sc\")\n",
    "\n",
    "## Plotting AF2 scores\n",
    "plt.figure(figsize=(12, 3))\n",
    "for i,k in enumerate(AF2_filters):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.hist(scores_af2[k])\n",
    "    plt.title(k)\n",
    "    plt.xlabel(k)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "    \n",
    "utils.plot_score_pairs(scores_af2, \"lDDT\", \"rmsd\", AF2_filters[\"lDDT\"][0], AF2_filters[\"rmsd\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afad819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Copying good predictions to a separate directory\n",
    "os.chdir(AF2_DIR)\n",
    "\n",
    "if len(scores_af2_filtered) > 0:\n",
    "    os.makedirs(\"good\", exist_ok=True)\n",
    "    good_af2_models = [row[\"Output_PDB\"]+\".pdb\" for idx,row in scores_af2_filtered.iterrows()]\n",
    "    for pdb in good_af2_models:\n",
    "        copy2(pdb, f\"good/{pdb}\")\n",
    "    good_af2_models = glob.glob(f\"{AF2_DIR}/good/*.pdb\")\n",
    "else:\n",
    "    sys.exit(\"No good models to continue this pipeline with\")\n",
    "\n",
    "os.chdir(f\"{AF2_DIR}/good\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d059ff96",
   "metadata": {},
   "source": [
    "## 6.1: Final FastRelax with the ligand\n",
    "Relaxing good AF2 models together with the ligand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7accf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "AF2_DIR = f\"{WDIR}/inpainting/5.1_2nd_af2\"\n",
    "DESIGN_DIR_ligMPNN = f\"{WDIR}/inpainting/3.1_design_pocket_ligandMPNN\"\n",
    "assert len(glob.glob(f\"{AF2_DIR}/good/*.pdb\")) > 0, \"No good AF2 models to relax with\"\n",
    "\n",
    "os.chdir(WDIR)\n",
    "RELAX_DIR = f\"{WDIR}/inpainting/6.1_final_relax\"\n",
    "os.makedirs(RELAX_DIR, exist_ok=True)\n",
    "os.chdir(RELAX_DIR)\n",
    "\n",
    "\n",
    "## First matching up the AF2 output filenames of step 5 with pocket design filenames from step 3\n",
    "ref_and_model_pairs = []\n",
    "for r in glob.glob(f\"{DESIGN_DIR_ligMPNN}/good/*.pdb\"):\n",
    "    for pdbfile in glob.glob(f\"{AF2_DIR}/good/*.pdb\"):\n",
    "        if os.path.basename(r).replace(\".pdb\", \"_\") in pdbfile:\n",
    "            ref_and_model_pairs.append((r, pdbfile))\n",
    "\n",
    "assert len(ref_and_model_pairs) == len(glob.glob(f\"{AF2_DIR}/good/*.pdb\")), \"Was not able to match all models with reference structures\"\n",
    "\n",
    "\n",
    "## Generating commands for relax jobs\n",
    "### Performing 1 relax iteration on each input structure\n",
    "NSTRUCT = 1\n",
    "cstfile = f\"{SCRIPT_DIR}/theozyme/HBA/HBA_CYS_UPO.cst\"\n",
    "\n",
    "commands_relax = []\n",
    "cmds_filename_rlx = \"commands_design\"\n",
    "with open(cmds_filename_rlx, \"w\") as file:\n",
    "    for r_m in ref_and_model_pairs:\n",
    "        commands_relax.append(f\"{PYTHON['general']} {SCRIPT_DIR}/scripts/design/align_add_ligand_relax.py \"\n",
    "                              f\"--outdir ./ --ligand {LIGAND} --ref_pdb {r_m[0]}\"\n",
    "                              f\"--pdb {r_m[1]} --nstruct {NSTRUCT} \"\n",
    "                              f\"--params {' '.join(params)} --cstfile {cstfile} > logs/{os.path.basename(pdb).replace('.pdb', '.log')}\\n\")\n",
    "        file.write(commands_relax[-1])\n",
    "\n",
    "print(\"Example design command:\")\n",
    "print(commands_design[-1])\n",
    "\n",
    "\n",
    "### Running design jobs with Slurm.\n",
    "submit_script = \"submit_relax.sh\"\n",
    "utils.create_slurm_submit_script(filename=submit_script, name=\"6.1_final_relax\", mem=\"4g\", \n",
    "                                 N_cores=1, time=\"0:30:00\", email=EMAIL, array=len(commands_relax),\n",
    "                                 array_commandfile=cmds_filename_rlx)\n",
    "\n",
    "if not os.path.exists(RELAX_DIR+\"/.done\"):\n",
    "    p = subprocess.Popen(['sbatch', submit_script], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    (output, err) = p.communicate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f57db8",
   "metadata": {},
   "source": [
    "### Analyzing final relaxed structures\n",
    "Filtering them based on the same metrics as was used for the initial design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b11d1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analyzing Rosetta designs\n",
    "RELAX_DIR = f\"{WDIR}/inpainting/6.1_final_relax\"\n",
    "os.chdir(RELAX_DIR)\n",
    "\n",
    "scores = pd.read_csv(\"scorefile.txt\", sep=\"\\s+\", header=0)\n",
    "\n",
    "filters = {'all_cst': [1.0, '<='],\n",
    " 'nlr_SR1_rms': [0.8, '<='],\n",
    " 'nlr_totrms': [1.0, '<='],\n",
    " 'L_SASA': [0.2, '<='],\n",
    " 'COO_hbond': [1.0, '='],\n",
    " 'heme_angle_wrst': [80.0, '>='],\n",
    " 'score_per_res': [0.0, '<='],\n",
    " 'corrected_ddg': [-50.0, '<='],\n",
    " 'cms_per_atom': [4.8, '>='],\n",
    " 'rmsd_CA_rlx_in': [1.0, \"<=\"]}  # rmsd_CA_rlx_in is rmsd between relaxed structure and AF2 prediction\n",
    "\n",
    "filtered_scores = utils.filter_scores(scores, filters)\n",
    "\n",
    "## Plotting relax scores\n",
    "plt.figure(figsize=(12, 9))\n",
    "for i,k in enumerate(filters):\n",
    "    if k not in scores.keys():\n",
    "        continue\n",
    "    plt.subplot(4, 3, i+1)\n",
    "    plt.hist(scores[k])\n",
    "    plt.title(k)\n",
    "    plt.xlabel(k)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "### Copying good designs over to a new directory\n",
    "if len(filtered_scores) > 0:\n",
    "    os.makedirs(f\"{RELAX_DIR}/good\", exist_ok=True)\n",
    "    for idx, row in filtered_scores.iterrows():\n",
    "        copy2(row[\"description\"]+\".pdb\", \"good/\"+row[\"description\"]+\".pdb\")\n",
    "else:\n",
    "    print(\"No good designs created, bummer...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba174b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(filtered_scores) > 0:\n",
    "    print(f\"CONGRATULATIONS! You have successfully designed {len(filtered_scores)} proteins against ligand {LIGAND}\")\n",
    "    print(\"You can find the design models in the directory:\\n\"\n",
    "          f\"    {RELAX_DIR}/good\")\n",
    "    print(\"\\nIt is advised you manually inspect them before ordering.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
